{"id": "data.tokenizers", "path": "/home/mattg/clone/allennlp/allennlp/data/tokenizers/__init__.py", "mtime": 1504298293, "size": 299, "hash": "bde589df9da433a1b146173dc851e544", "data_mtime": 1504544414, "dependencies": ["builtins"], "suppressed": ["allennlp.data.tokenizers.tokenizer", "allennlp.data.tokenizers.word_tokenizer", "allennlp.data.tokenizers.character_tokenizer"], "child_modules": ["data.tokenizers.word_splitter"], "options": {"strict_optional": false, "ignore_missing_imports": true, "no_implicit_optional": false, "warn_no_return": true, "ignore_errors": false, "disallow_any": [], "disallow_untyped_defs": false, "follow_imports": "normal", "disallow_untyped_calls": false, "platform": "linux", "check_untyped_defs": false, "show_none_errors": true, "strict_optional_whitelist": null, "quick_and_dirty": false, "strict_boolean": false, "warn_return_any": false}, "dep_prios": [5], "interface_hash": "3a79e3b88a969a1000c49634a6399fe9", "version_id": "0.521"}
