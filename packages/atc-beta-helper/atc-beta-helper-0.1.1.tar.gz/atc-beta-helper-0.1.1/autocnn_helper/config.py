# -*- coding: utf-8 -*-
from __future__ import absolute_import, division, print_function
import sys
import json
import os
import requests
import six


class DotDict(dict):
    __getattr__ = dict.__getitem__
    __setattr__ = dict.__setitem__
    __delattr__ = dict.__delitem__

    def __init__(self, dct):
        super(DotDict, self).__init__()
        for key, value in dct.items():
            if hasattr(value, 'keys'):
                value = DotDict(value)
            self[key] = value

    def __delitem__(self, key):
        super(DotDict, self).__delitem__(key)
        del self.__dict__[key]

    # setstate and getstate is for pickle
    def __getstate__(self):
        pass

    def __setstate__(self, *args, **kwargs):
        pass


def _load_json(str):
    if sys.version_info.major >= 3:
        return json.loads(str)

    def json_loads_byteified(json_text):
        return _byteify(
            json.loads(json_text, object_hook=_byteify),
            ignore_dicts=True
        )

    def _byteify(data, ignore_dicts=False):
        if isinstance(data, unicode):
            return data.encode('utf-8')
        if isinstance(data, list):
            return [_byteify(item, ignore_dicts=True) for item in data]
        if isinstance(data, dict) and not ignore_dicts:
            return {
                _byteify(key, ignore_dicts=True): _byteify(value, ignore_dicts=True)
                for key, value in data.iteritems()
            }
        return data
    return json_loads_byteified(str)


_parameter = None


def get_cluster_def():
    '''
    :return:
    {
        "master": ["master_uuid:2000"],
        "worker": ["worker_0_uuid:2000",
                   "worker_1_uuid:2000"],
        "ps": ["ps_uuid:2000"],
    }
    '''
    cluster = os.getenv('AUTOCNN_CLUSTER', None)
    try:
        return json.loads(cluster) if cluster else None
    except (ValueError, TypeError):
        print('Could get cluster definition, '
              'please make sure this is running inside a autocnn job.')
        return None


def get_parameter():
    global _parameter
    if _parameter is not None:
        return _parameter
    parameter = os.getenv('AUTOCNN_PARAMETER', None)
    try:
        param = DotDict(_load_json(parameter)) if parameter else None
        _parameter = param
        return _parameter
    except Exception as e:
        print('Could get parameters. {}'.format(e))
        return None


def write_parameter():
    argv = sys.argv
    if argv[1] == 'write' and len(argv) > 2:
        output_yml = argv[2]
    else:
        print('Write Failed !!!\nUsage `helper write yml_file_name` to write parameters to yml')
        sys.exit(1)
    parameter = os.getenv('AUTOCNN_PARAMETER', None)
    try:
        if parameter is None:
            raise ValueError('Parameter is None')
        path = os.path.dirname(output_yml)
        if path and not os.path.isdir(path):
            os.makedirs(path)
        with open(output_yml, 'w') as fo:
            fo.write(parameter)
    except Exception as e:
        print('Could get parameters. {}'.format(e))



def get_exp_info():
    """Returns the task info: {"type": "worker|master|ps", "index": int}."""
    info = os.getenv('AUTOCNN_EXP_INFO', None)
    try:
        return json.loads(info) if info else None
    except (ValueError, TypeError):
        print('Could get job info, '
              'please make sure this is running inside a autocnn job.')
        return None


def get_outputs_path():
    """The outputs path generated by autocnn based on the hierarchy of the task:

        `user/algorithm/group/task/files`
    """
    return os.getenv('AUTOCNN_OUTPUTS_PATH', None)


def get_data_path():
    """The data path generated by autocnn based on the hierarchy of the projet:

        `user/algorithm/`
    """
    return os.getenv('AUTOCNN_DATA_PATH', None)


def get_tf_config(envvar='TF_CONFIG'):
    """Returns the TF_CONFIG defining the cluster and the current task.

    if `envvar` is not null, it will set and env variable with `envvar`.
    """
    cluster_def = get_cluster_def()
    task_info = get_exp_info()
    tf_config = {
        'cluster': cluster_def,
        'task': task_info,
        'model_dir': get_outputs_path(),
        'environment': 'cloud'
    }

    if envvar:
        os.environ[envvar] = json.dumps(tf_config)

    return tf_config


def get_log_level():
    """If set on the autocnnfile it will return the log level."""
    return os.getenv('AUTOCNN_LOG_LEVEL', None)


def get_api(version='v1'):
    api = os.getenv('AUTOCNN_API', None)
    if not api:
        print('Could get api info, '
              'please make sure this is running inside a autocnn job.')
        return None
    return '{}/seetaas'.format(api)


def send_metrics(**metrics):
    """Sends metrics to autocnn api.

    Example:
        send_metric(precision=0.9, accuracy=0.89, loss=0.01)
    """
    exp_info = get_exp_info()
    exp_uuid = exp_info.get('exp_uuid', None)
    api = get_api()
    if not all([exp_uuid, api]):
        print('Environment information not found, '
              'please make sure this is running inside a autocnn job.')
        return

    try:
        formatted_metrics = {k: float(v) for k, v in six.iteritems(metrics)}
    except (ValueError, TypeError):
        print('Could not send metrics {}'.format(metrics))
        return

    try:
        s = requests.session()
        s.keep_alive = False
        s.post('{}/exp/metrics/{}'.format(api, exp_uuid),
                      data={'values': json.dumps(formatted_metrics)})
    except requests.RequestException as e:
        print('Could not reach autocnn api {}'.format(e))
